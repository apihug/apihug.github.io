---
title: ApiSmart 配置
description: ApiSmart 配置和支持的LLM供应商
---

ApiHug 的 ApiSmart 利用 [Langchain4j](https://github.com/langchain4j/langchain4j/) 与大型语言模型（LLM）供应商进行通信；因此基本上任何 Langchain4j 支持的供应商都可以被 ApiSmart 支持；

但由于我们目前的能力有限，我们需要逐个添加和测试，这将需要一些时间；

最终，所有这些供应商都能够进入我们的支持列表。

`Toolbar>ApiHug>Settings>AI`

## Settings


`Toolbar>ApiHug>Settings>AI>Settings`

常见的大型语言模型（LLM）运行时设置，它们都有默认值，如果不需要更改默认设置，则无需特别指定。

| Name              | Comment                                                                                                                                                                                                                                        | Mandatory |
|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|
| Chat Memory Size  | Memory window size, default 10                                                                                                                                                                                                                 | Y         |
| Top-P             | Top-P is a text generation method that focuses on the most likely tokens until a cumulative probability threshold is reached. It helps balance diversity and certainty in generated text. Use it with care, adjusting it for desired outcomes. | Y         |
| Max Output Tokens | Maximum attempts                                                                                                                                                                                                                               | Y         |
| Timeout(s)        | Time out in second                                                                                                                                                                                                                             | Y         |
| Max Attempts      | Max attempts                                                                                                                                                                                                                                   | Y         |
| Temperature       | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.                                                           | Y         |
| Stream            | Support stream                                                                                                                                                                                                                                 | Y         |
| System Prompt     | System prompt                                                                                                                                                                                                                                  | Y         |
| Show Detail       | Show time or token cost etc                                                                                                                                                                                                                    | Y         |


## Vendors

`Toolbar>ApiHug>Settings>AI>Vendor`

In theory [language-models](https://docs.langchain4j.dev/category/language-models) is the full list;

ApiSmart 支持您为同一个大型语言模型（LLM）供应商配置多个实例，例如用于不同的环境；

因此，每个设置都将具有特定的名称和描述，您应该使它们简单明了。例如，`Openai-test-env` 是一个很好的示例。


| Name        | Comment                               | Mandatory |
|-------------|---------------------------------------|-----------|
| Name        | Name of this LLM configuration        | Y         |
| Description | Description of this LLM configuration | N         |



### OpenAI

| Name         | Comment                              | Mandatory |
|--------------|--------------------------------------|-----------|
| URL          | `https://api.openai.com/` as default | N         |
| Api Key      | Key for the API                      | Y         |
| Project Id   | Not used yet                         | N         |
| Organization | Not used yet                         | N         |


### Azure AI

| Name         | Comment                                                         | Mandatory |
|--------------|-----------------------------------------------------------------|-----------|
| URL          | `https://***.openai.azure.com/` like endpoint                   | Y         |
| Api Key      | Key for the API                                                 | Y         |
| Model        | Default Model name(deployment), you can put manually on runtime | N         |


### Moonshot

| Name         | Comment                                                         | Mandatory |
|--------------|-----------------------------------------------------------------|-----------|
| URL          | `https://api.moonshot.cn/v1` like endpoint                   | Y         |
| Api Key      | Key for the API                                                 | Y         |
| Model        | Default Model name(deployment), you can put manually on runtime | N         |

- [moonshot api-key](https://platform.moonshot.cn/console/api-keys)
- [moonshot doc](https://platform.moonshot.cn/docs/intro)


### LLM Studio

| Name         | Comment                                                         | Mandatory |
|--------------|-----------------------------------------------------------------|-----------|
| URL          | `http://localhost:1234/v/1/` default local URL                  | Y         |

[lmstudio](https://lmstudio.ai/) Discover, download, and run local LLMs

[Local LLM Server](https://lmstudio.ai/docs/basics/server), You can serve local LLMs from LM Studio's Developer tab, either on localhost or on the network.

### Jan

| Name         | Comment                                                         | Mandatory |
|--------------|-----------------------------------------------------------------|-----------|
| URL          | `https://***.openai.azure.com/` like endpoint                   | Y         |

[Enable the Jan API Server](https://jan.ai/integrations/coding/continue-dev#step-2-enable-the-jan-api-server)

### OpenRouter

**TBD**

| Name         | Comment                                                         | Mandatory |
|--------------|-----------------------------------------------------------------|-----------|
| URL          | `https://***.openai.azure.com/` like endpoint                   | Y         |
| Api Key      | Key for the API                                                 | Y         |
| Model        | Default Model name(deployment), you can put manually on runtime | N         |


### Qianfan

| Name       | Comment                                              | Mandatory |
|------------|------------------------------------------------------|-----------|
| Api Key    | Key for the API                                      | Y         |
| Secret Key | Secret Key for the API                               | Y         |
| Model      | default model, you can input manually during runtime | Y         |

[百度智能云千帆大模型](https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application)

BTW `Yi-34B-Chat` is free;

### TBD

1. Google
2. ...

## Prompt

`Toolbar>ApiHug>Settings>AI>Prompt`

| Name   | Comment                                                                       | Mandatory |
|--------|-------------------------------------------------------------------------------|-----------|
| name   | shortcut style, can be refer `/{name}` when ask, `/explain` is a good example | Y         |
| prompt | body of the prompt                                                            | Y         |

⚠️保证 **name** 简短，唯一，易识别⚠️

---

This for ApiHug:

**BEGIN**

You are an experienced ApiHug developer skilled in OpenAPI design using ApiHug, a DSL for defining APIs with Protocol Buffers in a structured style.

For example, consider the following Proto definition:

```proto3
  rpc SayHello (google.protobuf.Empty) returns (google.protobuf.Empty) {
    option (hope.swagger.operation) = {
      get: "/say-hello";
      description: "Hello from ApiHug";
      tags: "project";
      priority: MIDDLE;
      pageable: true;
      authorization:{
        low_limit_risky_mode: ANONYMOUS
      }
    };
  };
```



This is equivalent to the standard OpenAPI definition:

```json

"/say-hello": {
      "get": {
        "summary": "Say Hello",
        "description": "Hello from ApiHug",
        "tags": ["project"],
        "responses": {
          "200": {
            "description": "Successful response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/Empty"
                }
              }
            }
          }
        },
        "security": [
          {
            "anonymous": []
          }
        ]
      }
    }
```

Some standard you need to follow:

1. Syntax must follow `proto3` standard
2. Post body define in rpc request while response in rpc return
3. Support pageable, or input output as plural
4. Always define the Message and Service separately

Please provide accurate and relevant information regarding OpenAPI and ApiHug code standards.

Do not include any unrelated information or references to external websites.

**END**
